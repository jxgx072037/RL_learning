{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd53a786",
   "metadata": {},
   "source": [
    "# 强化学习基础作业：马尔科夫决策过程（MDP）\n",
    "\n",
    "本次作业将帮助你理解和实现马尔科夫决策过程的基本概念。我们将通过一个简单的网格世界环境来实践MDP的核心组件。\n",
    "\n",
    "## 学习目标\n",
    "1. 理解MDP的五个基本要素：状态集、动作集、转移概率、奖励函数、折扣因子\n",
    "2. 实现一个简单的网格世界环境\n",
    "3. 计算状态价值函数和动作价值函数\n",
    "4. 实现策略评估和策略改进\n",
    "\n",
    "## 环境说明\n",
    "我们将使用一个4x4的网格世界作为我们的环境：\n",
    "- 智能体从起始位置(S)出发，目标是到达终点(G)\n",
    "- 网格中有一些障碍物(#)和陷阱(T)\n",
    "- 智能体可以采取上、下、左、右四个动作\n",
    "- 到达目标获得+1奖励，掉入陷阱获得-1奖励，其他步骤获得-0.04奖励\n",
    "\n",
    "```\n",
    "S  .  .  .\n",
    ".  #  .  .\n",
    ".  .  T  G\n",
    ".  T  .  .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814ea471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c08a9d",
   "metadata": {},
   "source": [
    "## 第1部分：环境实现\n",
    "\n",
    "首先，我们需要实现网格世界环境。这个环境需要包含以下功能：\n",
    "1. 初始化网格世界\n",
    "2. 定义状态转移函数\n",
    "3. 定义奖励函数\n",
    "4. 可视化功能\n",
    "\n",
    "### 任务1.1：实现GridWorld类\n",
    "请实现以下方法：\n",
    "- `__init__`: 初始化网格世界的基本参数\n",
    "- `get_next_state`: 根据当前状态和动作计算下一个状态\n",
    "- `get_reward`: 根据状态返回奖励\n",
    "- `is_terminal`: 判断是否为终止状态\n",
    "- `get_all_states`: 获取所有可能的状态\n",
    "- `visualize`: 可视化网格世界（可选择性地展示状态价值）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5480540f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self):\n",
    "        \"\"\"初始化网格世界环境\"\"\"\n",
    "        # TODO: 实现初始化方法\n",
    "        pass\n",
    "    \n",
    "    def get_next_state(self, state: Tuple[int, int], action: str) -> Tuple[int, int]:\n",
    "        \"\"\"根据当前状态和动作计算下一个状态\"\"\"\n",
    "        # TODO: 实现状态转移函数\n",
    "        pass\n",
    "    \n",
    "    def get_reward(self, state: Tuple[int, int]) -> float:\n",
    "        \"\"\"根据状态返回奖励\"\"\"\n",
    "        # TODO: 实现奖励函数\n",
    "        pass\n",
    "    \n",
    "    def is_terminal(self, state: Tuple[int, int]) -> bool:\n",
    "        \"\"\"判断是否为终止状态\"\"\"\n",
    "        # TODO: 实现终止状态判断\n",
    "        pass\n",
    "    \n",
    "    def get_all_states(self) -> List[Tuple[int, int]]:\n",
    "        \"\"\"获取所有可能的状态\"\"\"\n",
    "        # TODO: 实现获取所有状态的方法\n",
    "        pass\n",
    "    \n",
    "    def visualize(self, values: Dict[Tuple[int, int], float] = None):\n",
    "        \"\"\"可视化网格世界，可选择性地展示状态价值\"\"\"\n",
    "        # TODO: 实现可视化方法\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb7f35a",
   "metadata": {},
   "source": [
    "### 任务1.2：测试环境\n",
    "\n",
    "创建环境实例并测试基本功能：\n",
    "1. 创建GridWorld实例\n",
    "2. 测试可视化功能\n",
    "3. 测试状态转移和奖励计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b0acaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 创建环境实例并测试基本功能\n",
    "env = GridWorld()\n",
    "\n",
    "# 测试代码..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2a359d",
   "metadata": {},
   "source": [
    "## 第2部分：策略评估\n",
    "\n",
    "接下来，我们将实现策略评估算法。给定一个策略π，我们需要计算该策略下的状态价值函数V_π。\n",
    "\n",
    "策略评估的更新公式为：\n",
    "\n",
    "V_π(s) = Σ_a π(a|s) Σ_s' P(s'|s,a)[R(s,a,s') + γV_π(s')]\n",
    "\n",
    "在我们的简化版本中：\n",
    "- π是均匀随机策略（每个动作的概率相等）\n",
    "- P是确定性的（transition_prob = 1.0）\n",
    "- γ (gamma) 设为0.9\n",
    "\n",
    "### 任务2.1：实现策略评估算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb85286e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env: GridWorld, gamma: float = 0.9, theta: float = 1e-6) -> Dict[Tuple[int, int], float]:\n",
    "    \"\"\"\n",
    "    策略评估：计算均匀随机策略下的状态价值函数\n",
    "    \n",
    "    参数：\n",
    "        env: 网格世界环境\n",
    "        gamma: 折扣因子\n",
    "        theta: 收敛阈值\n",
    "    \n",
    "    返回：\n",
    "        状态价值函数字典 {state: value}\n",
    "    \"\"\"\n",
    "    # TODO: 实现策略评估算法\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d9674c",
   "metadata": {},
   "source": [
    "### 任务2.2：测试策略评估\n",
    "\n",
    "使用策略评估算法计算均匀随机策略下的状态价值函数，并可视化结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b3806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 运行策略评估并可视化结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7843d8f1",
   "metadata": {},
   "source": [
    "## 第3部分：最优策略\n",
    "\n",
    "现在，我们将实现价值迭代算法来找到最优策略。价值迭代的更新公式为：\n",
    "\n",
    "V*(s) = max_a Σ_s' P(s'|s,a)[R(s,a,s') + γV*(s')]\n",
    "\n",
    "### 任务3.1：实现价值迭代算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6698de0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env: GridWorld, gamma: float = 0.9, theta: float = 1e-6) -> Tuple[Dict[Tuple[int, int], float], Dict[Tuple[int, int], str]]:\n",
    "    \"\"\"\n",
    "    价值迭代算法\n",
    "    \n",
    "    参数：\n",
    "        env: 网格世界环境\n",
    "        gamma: 折扣因子\n",
    "        theta: 收敛阈值\n",
    "    \n",
    "    返回：\n",
    "        (最优价值函数, 最优策略)\n",
    "    \"\"\"\n",
    "    # TODO: 实现价值迭代算法\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3696116c",
   "metadata": {},
   "source": [
    "### 任务3.2：测试价值迭代\n",
    "\n",
    "使用价值迭代算法找到最优策略，并可视化结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a26820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 运行价值迭代并可视化结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594b1c11",
   "metadata": {},
   "source": [
    "## 第4部分：策略执行\n",
    "\n",
    "### 任务4.1：实现策略执行和可视化\n",
    "实现以下功能：\n",
    "1. 执行给定的策略并记录轨迹\n",
    "2. 可视化智能体的运动轨迹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1243015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_policy(env: GridWorld, policy: Dict[Tuple[int, int], str], max_steps: int = 100) -> List[Tuple[int, int]]:\n",
    "    \"\"\"执行给定的策略并返回轨迹\"\"\"\n",
    "    # TODO: 实现策略执行\n",
    "    pass\n",
    "\n",
    "def visualize_trajectory(env: GridWorld, trajectory: List[Tuple[int, int]]):\n",
    "    \"\"\"可视化智能体的运动轨迹\"\"\"\n",
    "    # TODO: 实现轨迹可视化\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d663404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 执行最优策略并可视化轨迹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aa9e79",
   "metadata": {},
   "source": [
    "## 思考题\n",
    "\n",
    "1. 为什么最优策略下的轨迹会选择这条路径？请结合奖励设置和折扣因子进行分析。\n",
    "\n",
    "2. 如果我们改变折扣因子γ的值（比如设为0.5或0.99），最优策略会有什么变化？为什么？\n",
    "\n",
    "3. 在当前的设置下，为什么有些状态的价值是负的？这些负价值状态对最优策略有什么影响？\n",
    "\n",
    "4. 如果我们想让智能体更倾向于选择更短的路径到达目标，应该如何修改奖励函数？请给出具体的修改建议。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
