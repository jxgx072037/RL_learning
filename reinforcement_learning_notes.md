# 强化学习学习笔记

## 目录
- [强化学习学习笔记](#强化学习学习笔记)
  - [目录](#目录)
  - [1. 强化学习基础概念](#1-强化学习基础概念)
    - [1.1 核心要素](#11-核心要素)
    - [1.2 基本工作流程](#12-基本工作流程)
  - [2. 马尔科夫链（Markov Chain）](#2-马尔科夫链markov-chain)
    - [2.1 基本定义](#21-基本定义)
    - [2.2 关键要素](#22-关键要素)
    - [2.3 性质](#23-性质)
  - [3. 马尔科夫奖励过程（Markov Reward Process, MRP）](#3-马尔科夫奖励过程markov-reward-process-mrp)
    - [3.1 在马尔科夫链基础上的扩展](#31-在马尔科夫链基础上的扩展)
    - [3.2 关键要素](#32-关键要素)
      - [3.2.1 为什么需要折扣因子 γ](#321-为什么需要折扣因子-γ)
    - [3.3 价值函数](#33-价值函数)
      - [3.3.1 价值函数的迭代计算](#331-价值函数的迭代计算)
      - [3.3.2 蒙特卡洛方法计算价值函数](#332-蒙特卡洛方法计算价值函数)
    - [3.4 贝尔曼方程（Bellman Equation）](#34-贝尔曼方程bellman-equation)
      - [3.4.0 关于Richard Bellman](#340-关于richard-bellman)
      - [3.4.1 贝尔曼方程与价值函数的关系](#341-贝尔曼方程与价值函数的关系)
      - [3.4.2 方程解析](#342-方程解析)
      - [3.4.3 实例说明](#343-实例说明)
        - [具体数值示例](#具体数值示例)
      - [3.4.4 贝尔曼方程的重要性](#344-贝尔曼方程的重要性)
  - [4. 马尔科夫决策过程（Markov Decision Process, MDP）](#4-马尔科夫决策过程markov-decision-process-mdp)
    - [4.1 在MRP基础上的扩展](#41-在mrp基础上的扩展)
    - [4.2 关键要素](#42-关键要素)
    - [4.3 策略](#43-策略)
    - [4.4 价值函数](#44-价值函数)
    - [4.5 价值迭代与策略迭代](#45-价值迭代与策略迭代)
      - [4.5.1 基本概念](#451-基本概念)
      - [4.5.2 具体例子：冰湖问题](#452-具体例子冰湖问题)
      - [4.5.3 两种方法的对比](#453-两种方法的对比)
  - [5. 三者关系](#5-三者关系)
    - [5.1 层层递进关系](#51-层层递进关系)
    - [5.2 应用视角](#52-应用视角)
  - [6. 实际应用举例](#6-实际应用举例)
    - [6.1 天气预测（马尔科夫链）](#61-天气预测马尔科夫链)
    - [6.2 股票投资（MRP）](#62-股票投资mrp)
    - [6.3 强化学习（MDP）](#63-强化学习mdp)

---

## 1. 强化学习基础概念

### 1.1 核心要素
- **智能体(Agent)**: 学习和做决策的主体
- **环境(Environment)**: 智能体所在的外部世界
- **状态(State)**: 环境的当前情况
- **动作(Action)**: 智能体可以采取的行为
- **奖励(Reward)**: 环境对智能体行为的反馈
- **策略(Policy)**: 智能体的行为准则，决定在某个状态下应该采取什么动作

### 1.2 基本工作流程
1. 智能体观察当前环境状态
2. 根据策略选择一个动作
3. 执行动作并获得奖励
4. 环境转移到新的状态
5. 重复以上步骤

## 2. 马尔科夫链（Markov Chain）

### 2.1 基本定义
- 马尔科夫链是最简单的马尔科夫随机过程
- 核心特征是**马尔科夫性质**：下一个状态的概率分布只取决于当前状态，而与之前的状态无关
- 数学表达：$P(S_{t+1} | S_t, S_{t-1}, ..., S_1) = P(S_{t+1} | S_t)$

### 2.2 关键要素
- **状态空间**: 系统可能处于的所有状态的集合
- **转移概率**: 从一个状态转移到另一个状态的概率
- **状态转移矩阵**: 包含所有状态之间转移概率的矩阵

### 2.3 性质
1. **无记忆性**: 未来的转移只依赖于当前状态
2. **时间同质性**: 转移概率不随时间变化
3. **可达性**: 从一个状态到达另一个状态的可能性
4. **常返性**: 从一个状态出发最终返回该状态的概率

## 3. 马尔科夫奖励过程（Markov Reward Process, MRP）

### 3.1 在马尔科夫链基础上的扩展
- 增加了**奖励函数** R(s)
- 增加了**折扣因子** γ (gamma)

### 3.2 关键要素
- **状态空间** S
- **转移概率矩阵** P
- **奖励函数** R(s)
- **折扣因子** γ ∈ [0,1]

#### 3.2.1 为什么需要折扣因子 γ
1. **避免循环马尔科夫过程中的无限回报**：
   - 在循环的状态转换中防止累积奖励无限增长

2. **处理未来的不确定性**：
   - 未来的状态和奖励可能无法被完全准确地预测
   - 越远的未来越不确定，因此需要打折扣

3. **金融收益的时间价值**：
   - 如果奖励代表金融收益，即时奖励可能比延迟奖励获得更多利息

4. **符合动物/人类行为特征**：
   - 生物普遍表现出对即时奖励的偏好

5. **特殊情况**：
   - 在某些情况下可以使用无折扣的马尔科夫奖励过程（即 γ = 1）
   - 例如：当所有序列都会终止时

6. **γ 的取值含义**：
   - γ = 0：只关心即时奖励
   - γ = 1：未来奖励与即时奖励同等重要

### 3.3 价值函数
- 定义：从状态s开始，未来可以获得的折扣奖励之和的期望
- 数学表达：$V(s) = \mathbb{E}[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ... | S_t = s]$

#### 3.3.1 价值函数的迭代计算
1. **算法步骤**：
   ```
   1. 对所有状态s∈S，初始化：V'(s)←0, V(s)←∞
   2. 当 ||V-V'|| > ε 时，重复：
      - V ← V'
      - 对所有状态s∈S：V'(s) = R(s) + γ∑[P(s'|s)V(s')]
   3. 返回V'(s)
   ```

2. **具体例子**：
假设有一个简单的天气系统，状态包括：晴天(s1)、多云(s2)、雨天(s3)

- **状态转移概率矩阵P**：
  ```
  P = [
    [0.7, 0.2, 0.1],  # 晴天转移概率
    [0.3, 0.4, 0.3],  # 多云转移概率
    [0.2, 0.3, 0.5]   # 雨天转移概率
  ]
  ```

- **即时奖励R**：
  - R(s1) = 5 (晴天)
  - R(s2) = 3 (多云)
  - R(s3) = 1 (雨天)

- **折扣因子**：γ = 0.9

3. **迭代计算过程**：

第0次迭代（初始化）：
```
V0(s1) = 0
V0(s2) = 0
V0(s3) = 0
```

第1次迭代：
```
V1(s1) = 5 + 0.9[0.7×0 + 0.2×0 + 0.1×0] = 5
V1(s2) = 3 + 0.9[0.3×0 + 0.4×0 + 0.3×0] = 3
V1(s3) = 1 + 0.9[0.2×0 + 0.3×0 + 0.5×0] = 1
```

第2次迭代：
```
V2(s1) = 5 + 0.9[0.7×5 + 0.2×3 + 0.1×1]
       = 5 + 0.9[3.5 + 0.6 + 0.1]
       = 5 + 0.9×4.2 = 8.78

V2(s2) = 3 + 0.9[0.3×5 + 0.4×3 + 0.3×1]
       = 3 + 0.9[1.5 + 1.2 + 0.3]
       = 3 + 0.9×3 = 5.7

V2(s3) = 1 + 0.9[0.2×5 + 0.3×3 + 0.5×1]
       = 1 + 0.9[1 + 0.9 + 0.5]
       = 1 + 0.9×2.4 = 3.16
```

4. **结果解释**：
   - 每次迭代都考虑了即时奖励和未来可能的状态转移
   - 价值会随着迭代次数增加而逐渐收敛
   - 最终的价值反映了：
     * 即时奖励的影响
     * 状态转移概率的影响
     * 折扣因子对未来奖励的衰减

5. **收敛条件**：
   - 当相邻两次迭代的价值差异小于阈值ε时停止
   - 在这个例子中，可以看到价值在逐步提升并趋于稳定

7. **价值迭代的收敛性**：
   价值迭代一定会收敛到唯一的最优价值函数，这是由以下原因保证的：

   - **理论基础**：基于"压缩映射定理"（Contraction Mapping Theorem）
   - **关键条件**：折扣因子 γ < 1 是保证收敛的必要条件
   - **收敛过程**：
     * 每次迭代时，远期奖励的影响会随着时间衰减
     * 第一步的影响是 γ
     * 第二步的影响是 γ²
     * 第三步的影响是 γ³
     * 影响力会随时间指数衰减

8. **贝尔曼操作符**：
   贝尔曼操作符是价值迭代的核心数学工具：

   - **定义**：是一个函数到函数的映射，将一个价值函数 V 映射到另一个价值函数 TV
   - **数学表达**：(TV)(s) = R(s) + γ * Σ P(s'|s)V(s')
   - **性质**：
     * 单调性：如果 V₁ ≤ V₂，则 TV₁ ≤ TV₂
     * 压缩性：|TV₁ - TV₂| ≤ γ|V₁ - V₂|
   - **作用**：
     * 定义了价值函数的更新规则
     * 保证了价值迭代的收敛性
     * 为强化学习提供了理论基础

9. **迭代过程的理解**：
   为什么在计算当前状态价值时使用上一轮迭代的值：

   - **问题本质**：
     * V(s) = R(s) + γ * Σ P(s'|s)V(s') 形成了状态间的相互依赖
     * 要计算一个状态的价值，需要知道其他状态的价值
   
   - **解决方案**：
     * 使用动态规划思想
     * 每次迭代使用"当前最好的估计"来更新
     * 通过多次迭代逐步接近真实价值

   - **具体例子**：
     在天气系统中：
     ```python
     # 第n次迭代的估计值：
     V_n(晴天) = 10
     V_n(多云) = 8
     V_n(雨天) = 6

     # 第n+1次迭代计算晴天的价值：
     V_n+1(晴天) = 5 + 0.9 * (0.7*V_n(晴天) + 0.2*V_n(多云) + 0.1*V_n(雨天))
     ```
     每次迭代都基于上一轮的估计来更新，最终会收敛到真实价值。

#### 3.3.2 蒙特卡洛方法计算价值函数
1. **基本思想**：
   - 通过多次采样来估计期望
   - 每次采样模拟一条完整的状态序列
   - 计算实际获得的回报
   - 多次采样的平均值作为价值估计

2. **关键参数选择**：
   - **采样次数(n_episodes)**：
     * 基于大数定律和中心极限定理
     * 通常选择1000次左右
     * 太少（<100）会导致方差大，结果不稳定
     * 太多（>10000）计算开销过大
     * 可以通过观察估计值的方差来调整

   - **路径长度(max_steps)**：
     * 与折扣因子γ密切相关
     * 经验法则：
       - γ < 0.5:    建议步长 20
       - γ ≈ 0.7:    建议步长 50
       - γ ≈ 0.9:    建议步长 100
       - γ > 0.95:   建议步长 200
     * 原理：γᵏ随k增大迅速衰减（如γ=0.9时，第100步影响为0.9¹⁰⁰≈0.000027）

3. **符号约定**：
   - 使用大写G表示累积回报（Return）
   - G来自"Gain"或"Goal"的首字母
   - 数学表达：G_t = R_t + γR_{t+1} + γ²R_{t+2} + ...
   - 这是强化学习领域的标准符号约定

2. **具体例子**：
仍然使用前面的天气系统：
- 状态：晴天(s1)、多云(s2)、雨天(s3)
- 转移概率矩阵和奖励与之前相同
- 折扣因子 γ = 0.9

3. **采样过程**：
假设我们从晴天(s1)开始，进行5次采样：

第1次采样：
```
路径：s1 → s1 → s2 → s3
奖励序列：5 + 0.9×5 + 0.9²×3 + 0.9³×1
总回报：5 + 4.5 + 2.43 + 0.729 = 12.659
```

第2次采样：
```
路径：s1 → s2 → s2 → s2
奖励序列：5 + 0.9×3 + 0.9²×3 + 0.9³×3
总回报：5 + 2.7 + 2.43 + 2.187 = 12.317
```

第3次采样：
```
路径：s1 → s3 → s2 → s1
奖励序列：5 + 0.9×1 + 0.9²×3 + 0.9³×5
总回报：5 + 0.9 + 2.43 + 3.645 = 11.975
```

第4次采样：
```
路径：s1 → s2 → s1 → s2
奖励序列：5 + 0.9×3 + 0.9²×5 + 0.9³×3
总回报：5 + 2.7 + 4.05 + 2.187 = 13.937
```

第5次采样：
```
路径：s1 → s1 → s3 → s2
奖励序列：5 + 0.9×5 + 0.9²×1 + 0.9³×3
总回报：5 + 4.5 + 0.81 + 2.187 = 12.497
```

4. **价值估计**：
```
V(s1) ≈ (12.659 + 12.317 + 11.975 + 13.937 + 12.497) / 5 = 12.677
```

5. **方法特点**：
   - **优点**：
     * 不需要完整的环境模型
     * 可以处理大状态空间
     * 易于实现和理解
   
   - **缺点**：
     * 需要多次采样才能得到准确估计
     * 采样方差可能较大
     * 计算成本随路径长度增加

6. **与动态规划方法对比**：
   - 动态规划需要完整的环境模型（转移概率和奖励）
   - 蒙特卡洛方法只需要能够采样
   - 动态规划计算更精确但计算量大
   - 蒙特卡洛方法可能不够精确但更灵活

7. **计算量比较分析**：
   - **影响因素**：
     * 折扣因子 γ 的大小
     * 蒙特卡洛采样的路径长度
     * 状态空间的大小
     * 所需的精确度（采样次数）

   - **价值迭代的计算量**：
     * 与 γ 值密切相关：γ 越小收敛越快
     * 每次迭代计算量 = 状态数 × 可能的转移数
     * 总计算量 = 迭代次数 × 每次迭代计算量

   - **蒙特卡洛方法的计算量**：
     * 与采样路径长度和采样次数相关
     * 总计算量 = 采样次数 × 路径长度
     * 路径越长，计算量越大

   - **结论**：
     当满足以下条件时，价值迭代的计算量会小于蒙特卡洛方法：
     * γ 较小（例如 < 0.5）
     * 蒙特卡洛采样路径长度较长（例如 > 50步）
     * 需要较多采样次数来保证准确性

   - **选择建议**：
     * 如果问题允许使用较小的 γ 值
     * 且需要考虑较长的时间步长
     * 那么优先选择价值迭代方法

### 3.4 贝尔曼方程（Bellman Equation）

- **定义**：贝尔曼方程描述了状态之间的迭代关系，是强化学习中最重要的方程之一
- **数学表达**：$V(s) = R(s) + \gamma \sum_{s' \in S} P(s'|s)V(s')$

#### 3.4.0 关于Richard Bellman
理查德·贝尔曼（Richard Ernest Bellman，1920-1984）是20世纪最具影响力的数学家之一：

- **学术背景**：
  * 布鲁克林出生，在布鲁克林学院和威斯康星大学学习数学
  * 1946年在普林斯顿大学获得博士学位
  * 导师是所罗门·勒夫谢茨（Solomon Lefschetz）

- **主要贡献**：
  * 发明了动态规划（Dynamic Programming）
  * 提出了贝尔曼方程（Bellman Equation）
  * 创造了"维数灾难"（Curse of Dimensionality）这个术语
  * 在控制论、动态规划和数学优化等领域有开创性贡献

- **研究特点**：
  * 善于将复杂问题分解为简单的子问题
  * 强调数学在实际问题中的应用
  * 提倡用优雅的数学方法解决工程问题

- **对强化学习的影响**：
  * 贝尔曼方程是现代强化学习的理论基础
  * 动态规划方法启发了众多强化学习算法
  * 他的工作使得复杂的序列决策问题可以被系统地解决

- **名言**：
  > "最优性原理：一个最优策略的任何子策略也是最优的。"
  
  这个原理是动态规划和强化学习的核心思想之一。

#### 3.4.1 贝尔曼方程与价值函数的关系
1. **本质联系**：
   - 贝尔曼方程是价值函数的递归定义
   - 价值函数 $V(s) = \mathbb{E}[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ... | S_t = s]$ 是一个无限序列
   - 贝尔曼方程将这个无限序列转化为递归形式：当前价值 = 即时奖励 + 折扣后的未来价值

2. **求解价值函数**：
   - 贝尔曼方程提供了计算价值函数的方法
   - 通过迭代求解贝尔曼方程可以得到价值函数的值
   - 这就是动态规划方法的基础

3. **直观理解**：
   - 价值函数告诉我们"一个状态值多少"
   - 贝尔曼方程告诉我们"如何计算这个值"
   - 价值函数是问题"是什么"，贝尔曼方程是问题"怎么算"

4. **实际应用**：
   - 在强化学习中，我们经常需要估计价值函数
   - 贝尔曼方程提供了估计的理论基础和实际方法
   - 很多强化学习算法（如Q-learning）都基于贝尔曼方程

#### 3.4.2 方程解析
1. **组成部分**：
   - $V(s)$: 当前状态s的价值
   - $R(s)$: 当前状态的即时奖励
   - $\gamma$: 折扣因子
   - $\sum_{s' \in S} P(s'|s)V(s')$: 所有可能的下一个状态的价值期望

2. **直观理解**：
   - 当前状态的价值 = 即时奖励 + 折扣后的未来状态价值期望
   - 体现了"当前决策要考虑未来影响"的思想

#### 3.4.3 实例说明
以图中的马尔科夫转移矩阵为例：
- 从状态s1出发：
  - 有0.1的概率留在s1
  - 有0.2的概率转移到s2
  - 有0.7的概率转移到s4
- 状态价值计算：
  ```
  V(s1) = R(s1) + γ[0.1×V(s1) + 0.2×V(s2) + 0.7×V(s4)]
  ```

##### 具体数值示例
假设我们有以下条件：
- 即时奖励 R(s1) = 5（在s1状态获得的即时奖励）
- 折扣因子 γ = 0.9
- 各状态的价值：
  - V(s1) = 20
  - V(s2) = 15
  - V(s4) = 10

代入贝尔曼方程计算s1的实际价值：
```
V(s1) = R(s1) + γ[0.1×V(s1) + 0.2×V(s2) + 0.7×V(s4)]
     = 5 + 0.9[0.1×20 + 0.2×15 + 0.7×10]
     = 5 + 0.9[2 + 3 + 7]
     = 5 + 0.9×12
     = 5 + 10.8
     = 15.8
```

这个例子说明：
1. **即时奖励部分**：
   - R(s1) = 5，这是立即获得的奖励

2. **未来价值期望部分**：
   - 0.1×20 = 2（留在s1的贡献）
   - 0.2×15 = 3（转移到s2的贡献）
   - 0.7×10 = 7（转移到s4的贡献）
   - 总期望 = 12
   - 折扣后 = 0.9×12 = 10.8

3. **最终价值**：
   - 即时奖励(5) + 折扣后的未来期望(10.8) = 15.8

这表明状态s1的总价值由两部分组成：
- 立即获得的奖励（5分）
- 未来可能获得的折扣奖励（10.8分）

#### 3.4.4 贝尔曼方程的重要性
1. **理论基础**：
   - 为价值函数提供了数学基础
   - 将无限步的价值计算转化为单步迭代

2. **实际应用**：
   - 是动态规划方法的基础
   - 指导了很多强化学习算法的设计
   - 帮助理解即时决策和长期收益的关系

## 4. 马尔科夫决策过程（Markov Decision Process, MDP）

### 4.1 在MRP基础上的扩展
- 增加了**动作空间** A
- 转移概率和奖励函数都与动作相关

### 4.2 关键要素
- **状态空间** $S$
- **动作空间** $A$
- **转移概率** $P(s'|s,a)$
- **奖励函数** $R(s,a)$
- **折扣因子** $\gamma$

### 4.3 策略
- **定义**: 在每个状态下选择动作的规则
- **确定性策略**: $\pi(s) \rightarrow a$
- **随机性策略**: $\pi(a|s) \rightarrow [0,1]$

### 4.4 价值函数
1. **状态价值函数（V函数）**
   - 数学定义：$V^\pi(s) = \mathbb{E}[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ... | S_t = s, \pi]$
   - 含义：在策略π下，从状态s开始能获得的期望总回报
   - 特点：
     * 只依赖于状态s
     * 已经考虑了策略π下所有可能的动作
     * 相当于对当前策略下所有动作的价值进行了"平均"

2. **动作价值函数（Q函数）**
   - 数学定义：$Q^\pi(s,a) = \mathbb{E}[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ... | S_t = s, A_t = a, \pi]$
   - 含义：在策略π下，从状态s开始执行动作a后能获得的期望总回报
   - 特点：
     * 同时依赖状态s和动作a
     * 能评估具体某个动作的价值
     * 更细粒度，知道每个动作的具体价值

3. **两者关系**
   - V函数可以通过Q函数计算得到：$V^\pi(s) = \sum_{a\in A} \pi(a|s)Q^\pi(s,a)$
   - 这个公式表明：状态的价值是该状态下所有动作价值的期望

4. **实际应用区别**
   - V函数用于：
     * 评估某个状态的总体价值
     * 比较不同状态的优劣
     * 当我们只关心状态好坏时使用

   - Q函数用于：
     * 评估在某状态下采取某动作的价值
     * 直接用于选择最优动作
     * 在需要决策时更有用

5. **具体例子**
   假设在下棋游戏中：
   - V(s)告诉你当前局面的好坏
   - Q(s,a)告诉你在当前局面下走某一步棋的好坏
   - 如果要决定下一步走法，Q函数更有用，因为它直接告诉你每个可能走法的价值

### 4.5 价值迭代与策略迭代
#### 4.5.1 基本概念
1. **价值迭代（Value Iteration）**：
   - 直接迭代更新价值函数
   - 隐式地改进策略
   - 每次迭代都计算最优动作值
   - 算法更简单，但收敛可能较慢

2. **策略迭代（Policy Iteration）**：
   - 交替进行策略评估和策略改进
   - 显式地维护和更新策略
   - 包含完整的策略评估过程
   - 每次迭代都保证策略改进

#### 4.5.2 具体例子：冰湖问题
假设有一个4×4的冰湖，智能体需要从起点到达终点：

1. **环境设置**：
```
起点(S)  冰面  冰面  冰面
冰面    冰面  坑洞  冰面
冰面    冰面  冰面  坑洞
冰面    坑洞  冰面  终点(G)
```
- 状态：16个格子位置
- 动作：上、下、左、右
- 奖励：到达终点+1，掉入坑洞-1，其他步骤0
- 冰面湿滑：动作可能失败（0.8概率到达目标方向，0.1概率分别到达左右两侧）

2. **价值迭代过程**：
```
第1次迭代：
- 只考虑即时奖励
- 终点格子价值为1
- 坑洞格子价值为-1
- 其他格子价值为0

第2次迭代：
- 考虑一步可达的奖励
- 终点附近的格子开始显示正价值
- 坑洞附近的格子显示负价值

最终收敛：
- 从起点到终点的最优路径显现
- 每个格子的价值反映到达终点的可能性
```

3. **策略迭代过程**：
```
初始策略：
- 随机策略（每个动作概率0.25）

策略评估：
- 计算当前策略下每个状态的价值
- 多次迭代直至价值收敛

策略改进：
- 根据当前价值选择最优动作
- 更新策略（确定性或概率性）

重复评估和改进直至收敛
```

#### 4.5.3 两种方法的对比
1. **计算效率**：
   - 价值迭代：每步计算量小，但可能需要更多迭代
   - 策略迭代：每步计算量大，但通常迭代次数少

2. **收敛特性**：
   - 价值迭代：渐进收敛到最优值
   - 策略迭代：每次迭代都保证策略改进

3. **适用场景**：
   - 价值迭代：状态空间较大时更适用
   - 策略迭代：需要稳定策略更新时更适用

4. **实现复杂度**：
   - 价值迭代：实现简单，容易调试
   - 策略迭代：实现较复杂，需要额外存储策略

## 5. 三者关系
```
马尔科夫链 → 马尔科夫奖励过程 → 马尔科夫决策过程
(状态转移)  (增加奖励和折扣)  (增加动作和策略)
```

### 5.1 层层递进关系
1. 马尔科夫链是最基础的，只考虑状态转移
2. MRP在马尔科夫链基础上增加了奖励的概念
3. MDP在MRP基础上增加了动作的选择

### 5.2 应用视角
- **马尔科夫链**: 用于描述随机过程
- **MRP**: 用于评估固定策略的价值
- **MDP**: 用于寻找最优策略

## 6. 实际应用举例

### 6.1 天气预测（马尔科夫链）
- 状态：晴天、阴天、雨天
- 只需考虑状态转移概率

### 6.2 股票投资（MRP）
- 状态：股票价格
- 奖励：每日收益
- 不考虑主动决策

### 6.3 强化学习（MDP）
- 状态：环境状态
- 动作：智能体的选择
- 奖励：环境反馈
- 目标：找到最优策略