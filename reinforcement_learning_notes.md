# 强化学习学习笔记

## 目录
1. [强化学习基础概念](#1-强化学习基础概念)
   - [核心要素](#11-核心要素)
   - [基本工作流程](#12-基本工作流程)

2. [马尔科夫链（Markov Chain）](#2-马尔科夫链markov-chain)
   - [基本定义](#21-基本定义)
   - [关键要素](#22-关键要素)
   - [性质](#23-性质)

3. [马尔科夫奖励过程（MRP）](#3-马尔科夫奖励过程markov-reward-process-mrp)
   - [扩展内容](#31-在马尔科夫链基础上的扩展)
   - [关键要素](#32-关键要素)
     - [折扣因子说明](#321-为什么需要折扣因子-γ)
   - [价值函数](#33-价值函数)
   - [贝尔曼方程](#34-贝尔曼方程bellman-equation)
     - [与价值函数的关系](#341-贝尔曼方程与价值函数的关系)
     - [方程解析](#342-方程解析)
     - [实例说明](#343-实例说明)
     - [重要性](#344-贝尔曼方程的重要性)

4. [马尔科夫决策过程（MDP）](#4-马尔科夫决策过程markov-decision-process-mdp)
   - [扩展内容](#41-在mrp基础上的扩展)
   - [关键要素](#42-关键要素)
   - [策略](#43-策略)
   - [价值函数](#44-价值函数)

5. [三者关系](#5-三者关系)
   - [层层递进关系](#51-层层递进关系)
   - [应用视角](#52-应用视角)

6. [实际应用举例](#6-实际应用举例)
    - [天气预测](#61-天气预测马尔科夫链)
    - [股票投资](#62-股票投资mrp)
    - [强化学习](#63-强化学习mdp)

---

## 1. 强化学习基础概念

### 1.1 核心要素
- **智能体(Agent)**: 学习和做决策的主体
- **环境(Environment)**: 智能体所在的外部世界
- **状态(State)**: 环境的当前情况
- **动作(Action)**: 智能体可以采取的行为
- **奖励(Reward)**: 环境对智能体行为的反馈
- **策略(Policy)**: 智能体的行为准则，决定在某个状态下应该采取什么动作

### 1.2 基本工作流程
1. 智能体观察当前环境状态
2. 根据策略选择一个动作
3. 执行动作并获得奖励
4. 环境转移到新的状态
5. 重复以上步骤

## 2. 马尔科夫链（Markov Chain）

### 2.1 基本定义
- 马尔科夫链是最简单的马尔科夫随机过程
- 核心特征是**马尔科夫性质**：下一个状态的概率分布只取决于当前状态，而与之前的状态无关
- 数学表达：$P(S_{t+1} | S_t, S_{t-1}, ..., S_1) = P(S_{t+1} | S_t)$

### 2.2 关键要素
- **状态空间**: 系统可能处于的所有状态的集合
- **转移概率**: 从一个状态转移到另一个状态的概率
- **状态转移矩阵**: 包含所有状态之间转移概率的矩阵

### 2.3 性质
1. **无记忆性**: 未来的转移只依赖于当前状态
2. **时间同质性**: 转移概率不随时间变化
3. **可达性**: 从一个状态到达另一个状态的可能性
4. **常返性**: 从一个状态出发最终返回该状态的概率

## 3. 马尔科夫奖励过程（Markov Reward Process, MRP）

### 3.1 在马尔科夫链基础上的扩展
- 增加了**奖励函数** R(s)
- 增加了**折扣因子** γ (gamma)

### 3.2 关键要素
- **状态空间** S
- **转移概率矩阵** P
- **奖励函数** R(s)
- **折扣因子** γ ∈ [0,1]

#### 3.2.1 为什么需要折扣因子 γ
1. **避免循环马尔科夫过程中的无限回报**：
   - 在循环的状态转换中防止累积奖励无限增长

2. **处理未来的不确定性**：
   - 未来的状态和奖励可能无法被完全准确地预测
   - 越远的未来越不确定，因此需要打折扣

3. **金融收益的时间价值**：
   - 如果奖励代表金融收益，即时奖励可能比延迟奖励获得更多利息

4. **符合动物/人类行为特征**：
   - 生物普遍表现出对即时奖励的偏好

5. **特殊情况**：
   - 在某些情况下可以使用无折扣的马尔科夫奖励过程（即 γ = 1）
   - 例如：当所有序列都会终止时

6. **γ 的取值含义**：
   - γ = 0：只关心即时奖励
   - γ = 1：未来奖励与即时奖励同等重要

### 3.3 价值函数
- 定义：从状态s开始，未来可以获得的折扣奖励之和的期望
- 数学表达：$V(s) = \mathbb{E}[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ... | S_t = s]$

### 3.4 贝尔曼方程（Bellman Equation）
- **定义**：贝尔曼方程描述了状态之间的迭代关系，是强化学习中最重要的方程之一
- **数学表达**：$V(s) = R(s) + \gamma \sum_{s' \in S} P(s'|s)V(s')$

#### 3.4.1 贝尔曼方程与价值函数的关系
1. **本质联系**：
   - 贝尔曼方程是价值函数的递归定义
   - 价值函数 $V(s) = \mathbb{E}[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ... | S_t = s]$ 是一个无限序列
   - 贝尔曼方程将这个无限序列转化为递归形式：当前价值 = 即时奖励 + 折扣后的未来价值

2. **求解价值函数**：
   - 贝尔曼方程提供了计算价值函数的方法
   - 通过迭代求解贝尔曼方程可以得到价值函数的值
   - 这就是动态规划方法的基础

3. **直观理解**：
   - 价值函数告诉我们"一个状态值多少"
   - 贝尔曼方程告诉我们"如何计算这个值"
   - 价值函数是问题"是什么"，贝尔曼方程是问题"怎么算"

4. **实际应用**：
   - 在强化学习中，我们经常需要估计价值函数
   - 贝尔曼方程提供了估计的理论基础和实际方法
   - 很多强化学习算法（如Q-learning）都基于贝尔曼方程

#### 3.4.2 方程解析
1. **组成部分**：
   - $V(s)$: 当前状态s的价值
   - $R(s)$: 当前状态的即时奖励
   - $\gamma$: 折扣因子
   - $\sum_{s' \in S} P(s'|s)V(s')$: 所有可能的下一个状态的价值期望

2. **直观理解**：
   - 当前状态的价值 = 即时奖励 + 折扣后的未来状态价值期望
   - 体现了"当前决策要考虑未来影响"的思想

#### 3.4.3 实例说明
以图中的马尔科夫转移矩阵为例：
- 从状态s1出发：
  - 有0.1的概率留在s1
  - 有0.2的概率转移到s2
  - 有0.7的概率转移到s4
- 状态价值计算：
  ```
  V(s1) = R(s1) + γ[0.1×V(s1) + 0.2×V(s2) + 0.7×V(s4)]
  ```

##### 具体数值示例
假设我们有以下条件：
- 即时奖励 R(s1) = 5（在s1状态获得的即时奖励）
- 折扣因子 γ = 0.9
- 各状态的价值：
  - V(s1) = 20
  - V(s2) = 15
  - V(s4) = 10

代入贝尔曼方程计算s1的实际价值：
```
V(s1) = R(s1) + γ[0.1×V(s1) + 0.2×V(s2) + 0.7×V(s4)]
     = 5 + 0.9[0.1×20 + 0.2×15 + 0.7×10]
     = 5 + 0.9[2 + 3 + 7]
     = 5 + 0.9×12
     = 5 + 10.8
     = 15.8
```

这个例子说明：
1. **即时奖励部分**：
   - R(s1) = 5，这是立即获得的奖励

2. **未来价值期望部分**：
   - 0.1×20 = 2（留在s1的贡献）
   - 0.2×15 = 3（转移到s2的贡献）
   - 0.7×10 = 7（转移到s4的贡献）
   - 总期望 = 12
   - 折扣后 = 0.9×12 = 10.8

3. **最终价值**：
   - 即时奖励(5) + 折扣后的未来期望(10.8) = 15.8

这表明状态s1的总价值由两部分组成：
- 立即获得的奖励（5分）
- 未来可能获得的折扣奖励（10.8分）

#### 3.4.4 贝尔曼方程的重要性
1. **理论基础**：
   - 为价值函数提供了数学基础
   - 将无限步的价值计算转化为单步迭代

2. **实际应用**：
   - 是动态规划方法的基础
   - 指导了很多强化学习算法的设计
   - 帮助理解即时决策和长期收益的关系

## 4. 马尔科夫决策过程（Markov Decision Process, MDP）

### 4.1 在MRP基础上的扩展
- 增加了**动作空间** A
- 转移概率和奖励函数都与动作相关

### 4.2 关键要素
- **状态空间** $S$
- **动作空间** $A$
- **转移概率** $P(s'|s,a)$
- **奖励函数** $R(s,a)$
- **折扣因子** $\gamma$

### 4.3 策略
- **定义**: 在每个状态下选择动作的规则
- **确定性策略**: $\pi(s) \rightarrow a$
- **随机性策略**: $\pi(a|s) \rightarrow [0,1]$

### 4.4 价值函数
1. **状态价值函数**
   - $V^\pi(s) = \mathbb{E}[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ... | S_t = s, \pi]$

2. **动作价值函数**
   - $Q^\pi(s,a) = \mathbb{E}[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ... | S_t = s, A_t = a, \pi]$

## 5. 三者关系
```
马尔科夫链 → 马尔科夫奖励过程 → 马尔科夫决策过程
(状态转移)  (增加奖励和折扣)  (增加动作和策略)
```

### 5.1 层层递进关系
1. 马尔科夫链是最基础的，只考虑状态转移
2. MRP在马尔科夫链基础上增加了奖励的概念
3. MDP在MRP基础上增加了动作的选择

### 5.2 应用视角
- **马尔科夫链**: 用于描述随机过程
- **MRP**: 用于评估固定策略的价值
- **MDP**: 用于寻找最优策略

## 6. 实际应用举例

### 6.1 天气预测（马尔科夫链）
- 状态：晴天、阴天、雨天
- 只需考虑状态转移概率

### 6.2 股票投资（MRP）
- 状态：股票价格
- 奖励：每日收益
- 不考虑主动决策

### 6.3 强化学习（MDP）
- 状态：环境状态
- 动作：智能体的选择
- 奖励：环境反馈
- 目标：找到最优策略